{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvpnCX6IVtotrolTKffQ8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erindakapllani/question_generator/blob/main/questiongenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from typing import Any, List, Mapping, Tuple\n",
        "\n",
        "\n",
        "class QuestionGenerator:\n",
        "    \"\"\"A transformer-based NLP system for generating reading comprehension-style questions from\n",
        "    texts. It can generate full sentence questions, multiple choice questions, or a mix of the\n",
        "    two styles.\n",
        "\n",
        "    To filter out low quality questions, questions are assigned a score and ranked once they have\n",
        "    been generated. Only the top k questions will be returned. This behaviour can be turned off\n",
        "    by setting use_evaluator=False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        QG_PRETRAINED = \"iarfmoose/t5-base-question-generator\"\n",
        "        self.ANSWER_TOKEN = \"<answer>\"\n",
        "        self.CONTEXT_TOKEN = \"<context>\"\n",
        "        self.SEQ_LENGTH = 512\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.qg_tokenizer = AutoTokenizer.from_pretrained(\n",
        "            QG_PRETRAINED, use_fast=False)\n",
        "        self.qg_model = AutoModelForSeq2SeqLM.from_pretrained(QG_PRETRAINED)\n",
        "        self.qg_model.to(self.device)\n",
        "        self.qg_model.eval()\n",
        "\n",
        "        self.qa_evaluator = QAEvaluator()\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        article: str,\n",
        "        use_evaluator: bool = True,\n",
        "        num_questions: bool = None,\n",
        "        answer_style: str = \"all\"\n",
        "    ) -> List:\n",
        "        \"\"\"Takes an article and generates a set of question and answer pairs. If use_evaluator\n",
        "        is True then QA pairs will be ranked and filtered based on their quality. answer_style\n",
        "        should selected from [\"all\", \"sentences\", \"multiple_choice\"].\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Generating questions...\\n\")\n",
        "\n",
        "        qg_inputs, qg_answers = self.generate_qg_inputs(article, answer_style)\n",
        "        generated_questions = self.generate_questions_from_inputs(qg_inputs)\n",
        "\n",
        "        message = \"{} questions doesn't match {} answers\".format(\n",
        "            len(generated_questions), len(qg_answers)\n",
        "        )\n",
        "        assert len(generated_questions) == len(qg_answers), message\n",
        "\n",
        "        if use_evaluator:\n",
        "            print(\"Evaluating QA pairs...\\n\")\n",
        "            encoded_qa_pairs = self.qa_evaluator.encode_qa_pairs(\n",
        "                generated_questions, qg_answers\n",
        "            )\n",
        "            scores = self.qa_evaluator.get_scores(encoded_qa_pairs)\n",
        "\n",
        "            if num_questions:\n",
        "                qa_list = self._get_ranked_qa_pairs(\n",
        "                    generated_questions, qg_answers, scores, num_questions\n",
        "                )\n",
        "            else:\n",
        "                qa_list = self._get_ranked_qa_pairs(\n",
        "                    generated_questions, qg_answers, scores\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            print(\"Skipping evaluation step.\\n\")\n",
        "            qa_list = self._get_all_qa_pairs(generated_questions, qg_answers)\n",
        "\n",
        "        return qa_list\n",
        "\n",
        "    def generate_qg_inputs(self, text: str, answer_style: str) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Given a text, returns a list of model inputs and a list of corresponding answers.\n",
        "        Model inputs take the form \"answer_token <answer text> context_token <context text>\" where\n",
        "        the answer is a string extracted from the text, and the context is the wider text surrounding\n",
        "        the context.\n",
        "        \"\"\"\n",
        "\n",
        "        VALID_ANSWER_STYLES = [\"all\", \"sentences\", \"multiple_choice\"]\n",
        "\n",
        "        if answer_style not in VALID_ANSWER_STYLES:\n",
        "            raise ValueError(\n",
        "                \"Invalid answer style {}. Please choose from {}\".format(\n",
        "                    answer_style, VALID_ANSWER_STYLES\n",
        "                )\n",
        "            )\n",
        "\n",
        "        inputs = []\n",
        "        answers = []\n",
        "\n",
        "        if answer_style == \"sentences\" or answer_style == \"all\":\n",
        "            segments = self._split_into_segments(text)\n",
        "\n",
        "            for segment in segments:\n",
        "                sentences = self._split_text(segment)\n",
        "                prepped_inputs, prepped_answers = self._prepare_qg_inputs(\n",
        "                    sentences, segment\n",
        "                )\n",
        "                inputs.extend(prepped_inputs)\n",
        "                answers.extend(prepped_answers)\n",
        "\n",
        "        if answer_style == \"multiple_choice\" or answer_style == \"all\":\n",
        "            sentences = self._split_text(text)\n",
        "            prepped_inputs, prepped_answers = self._prepare_qg_inputs_MC(\n",
        "                sentences\n",
        "            )\n",
        "            inputs.extend(prepped_inputs)\n",
        "            answers.extend(prepped_answers)\n",
        "\n",
        "        return inputs, answers\n",
        "\n",
        "    def generate_questions_from_inputs(self, qg_inputs: List) -> List[str]:\n",
        "        \"\"\"Given a list of concatenated answers and contexts, with the form:\n",
        "        \"answer_token <answer text> context_token <context text>\", generates a list of\n",
        "        questions.\n",
        "        \"\"\"\n",
        "        generated_questions = []\n",
        "\n",
        "        for qg_input in qg_inputs:\n",
        "            question = self._generate_question(qg_input)\n",
        "            generated_questions.append(question)\n",
        "\n",
        "        return generated_questions\n",
        "\n",
        "    def _split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Splits the text into sentences, and attempts to split or truncate long sentences.\"\"\"\n",
        "        MAX_SENTENCE_LEN = 128\n",
        "        sentences = re.findall(\".*?[.!\\?]\", text)\n",
        "        cut_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) > MAX_SENTENCE_LEN:\n",
        "                cut_sentences.extend(re.split(\"[,;:)]\", sentence))\n",
        "\n",
        "        # remove useless post-quote sentence fragments\n",
        "        cut_sentences = [s for s in sentences if len(s.split(\" \")) > 5]\n",
        "        sentences = sentences + cut_sentences\n",
        "\n",
        "        return list(set([s.strip(\" \") for s in sentences]))\n",
        "\n",
        "    def _split_into_segments(self, text: str) -> List[str]:\n",
        "        \"\"\"Splits a long text into segments short enough to be input into the transformer network.\n",
        "        Segments are used as context for question generation.\n",
        "        \"\"\"\n",
        "        MAX_TOKENS = 490\n",
        "        paragraphs = text.split(\"\\n\")\n",
        "        tokenized_paragraphs = [\n",
        "            self.qg_tokenizer(p)[\"input_ids\"] for p in paragraphs if len(p) > 0\n",
        "        ]\n",
        "        segments = []\n",
        "\n",
        "        while len(tokenized_paragraphs) > 0:\n",
        "            segment = []\n",
        "\n",
        "            while len(segment) < MAX_TOKENS and len(tokenized_paragraphs) > 0:\n",
        "                paragraph = tokenized_paragraphs.pop(0)\n",
        "                segment.extend(paragraph)\n",
        "            segments.append(segment)\n",
        "\n",
        "        return [self.qg_tokenizer.decode(s, skip_special_tokens=True) for s in segments]\n",
        "\n",
        "    def _prepare_qg_inputs(\n",
        "        self,\n",
        "        sentences: List[str],\n",
        "        text: str\n",
        "    ) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Uses sentences as answers and the text as context. Returns a tuple of (model inputs, answers).\n",
        "        Model inputs are \"answer_token <answer text> context_token <context text>\"\n",
        "        \"\"\"\n",
        "        inputs = []\n",
        "        answers = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            qg_input = f\"{self.ANSWER_TOKEN} {sentence} {self.CONTEXT_TOKEN} {text}\"\n",
        "            inputs.append(qg_input)\n",
        "            answers.append(sentence)\n",
        "\n",
        "        return inputs, answers\n",
        "\n",
        "    def _prepare_qg_inputs_MC(self, sentences: List[str]) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Performs NER on the text, and uses extracted entities are candidate answers for multiple-choice\n",
        "        questions. Sentences are used as context, and entities as answers. Returns a tuple of (model inputs, answers).\n",
        "        Model inputs are \"answer_token <answer text> context_token <context text>\"\n",
        "        \"\"\"\n",
        "        spacy_nlp = en_core_web_sm.load()\n",
        "        docs = list(spacy_nlp.pipe(sentences, disable=[\"parser\"]))\n",
        "        inputs_from_text = []\n",
        "        answers_from_text = []\n",
        "\n",
        "        for doc, sentence in zip(docs, sentences):\n",
        "            entities = doc.ents\n",
        "            if entities:\n",
        "\n",
        "                for entity in entities:\n",
        "                    qg_input = f\"{self.ANSWER_TOKEN} {entity} {self.CONTEXT_TOKEN} {sentence}\"\n",
        "                    answers = self._get_MC_answers(entity, docs)\n",
        "                    inputs_from_text.append(qg_input)\n",
        "                    answers_from_text.append(answers)\n",
        "\n",
        "        return inputs_from_text, answers_from_text\n",
        "\n",
        "    def _get_MC_answers(self, correct_answer: Any, docs: Any) -> List[Mapping[str, Any]]:\n",
        "        \"\"\"Finds a set of alternative answers for a multiple-choice question. Will attempt to find\n",
        "        alternatives of the same entity type as correct_answer if possible.\n",
        "        \"\"\"\n",
        "        entities = []\n",
        "\n",
        "        for doc in docs:\n",
        "            entities.extend([{\"text\": e.text, \"label_\": e.label_}\n",
        "                            for e in doc.ents])\n",
        "\n",
        "        # remove duplicate elements\n",
        "        entities_json = [json.dumps(kv) for kv in entities]\n",
        "        pool = set(entities_json)\n",
        "        num_choices = (\n",
        "            min(4, len(pool)) - 1\n",
        "        )  # -1 because we already have the correct answer\n",
        "\n",
        "        # add the correct answer\n",
        "        final_choices = []\n",
        "        correct_label = correct_answer.label_\n",
        "        final_choices.append({\"answer\": correct_answer.text, \"correct\": True})\n",
        "        pool.remove(\n",
        "            json.dumps({\"text\": correct_answer.text,\n",
        "                       \"label_\": correct_answer.label_})\n",
        "        )\n",
        "\n",
        "        # find answers with the same NER label\n",
        "        matches = [e for e in pool if correct_label in e]\n",
        "\n",
        "        # if we don't have enough then add some other random answers\n",
        "        if len(matches) < num_choices:\n",
        "            choices = matches\n",
        "            pool = pool.difference(set(choices))\n",
        "            choices.extend(random.sample(pool, num_choices - len(choices)))\n",
        "        else:\n",
        "            choices = random.sample(matches, num_choices)\n",
        "\n",
        "        choices = [json.loads(s) for s in choices]\n",
        "\n",
        "        for choice in choices:\n",
        "            final_choices.append({\"answer\": choice[\"text\"], \"correct\": False})\n",
        "\n",
        "        random.shuffle(final_choices)\n",
        "        return final_choices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_question(self, qg_input: str) -> str:\n",
        "        \"\"\"Takes qg_input which is the concatenated answer and context, and uses it to generate\n",
        "        a question sentence. The generated question is decoded and then returned.\n",
        "        \"\"\"\n",
        "        encoded_input = self._encode_qg_input(qg_input)\n",
        "        output = self.qg_model.generate(input_ids=encoded_input[\"input_ids\"])\n",
        "        question = self.qg_tokenizer.decode(\n",
        "            output[0],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        return question\n",
        "\n",
        "    def _encode_qg_input(self, qg_input: str) -> torch.tensor:\n",
        "        \"\"\"Tokenizes a string and returns a tensor of input ids corresponding to indices of tokens in\n",
        "        the vocab.\n",
        "        \"\"\"\n",
        "        return self.qg_tokenizer(\n",
        "            qg_input,\n",
        "            padding='max_length',\n",
        "            max_length=self.SEQ_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "    def _get_ranked_qa_pairs(\n",
        "        self, generated_questions: List[str], qg_answers: List[str], scores, num_questions: int = 10\n",
        "    ) -> List[Mapping[str, str]]:\n",
        "        \"\"\"Ranks generated questions according to scores, and returns the top num_questions examples.\n",
        "        \"\"\"\n",
        "        if num_questions > len(scores):\n",
        "            num_questions = len(scores)\n",
        "            print((\n",
        "                f\"\\nWas only able to generate {num_questions} questions.\",\n",
        "                \"For more questions, please input a longer text.\")\n",
        "            )\n",
        "\n",
        "        qa_list = []\n",
        "\n",
        "        for i in range(num_questions):\n",
        "            index = scores[i]\n",
        "            qa = {\n",
        "                \"question\": generated_questions[index].split(\"?\")[0] + \"?\",\n",
        "                \"answer\": qg_answers[index]\n",
        "            }\n",
        "            qa_list.append(qa)\n",
        "\n",
        "        return qa_list\n",
        "\n",
        "    def _get_all_qa_pairs(self, generated_questions: List[str], qg_answers: List[str]):\n",
        "        \"\"\"Formats question and answer pairs without ranking or filtering.\"\"\"\n",
        "        qa_list = []\n",
        "\n",
        "        for question, answer in zip(generated_questions, qg_answers):\n",
        "            qa = {\n",
        "                \"question\": question.split(\"?\")[0] + \"?\",\n",
        "                \"answer\": answer\n",
        "            }\n",
        "            qa_list.append(qa)\n",
        "\n",
        "        return qa_list\n",
        "\n",
        "\n",
        "class QAEvaluator:\n",
        "    \"\"\"Wrapper for a transformer model which evaluates the quality of question-answer pairs.\n",
        "    Given a QA pair, the model will generate a score. Scores can be used to rank and filter\n",
        "    QA pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        QAE_PRETRAINED = \"iarfmoose/bert-base-cased-qa-evaluator\"\n",
        "        self.SEQ_LENGTH = 512\n",
        "\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.qae_tokenizer = AutoTokenizer.from_pretrained(QAE_PRETRAINED)\n",
        "        self.qae_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            QAE_PRETRAINED\n",
        "        )\n",
        "        self.qae_model.to(self.device)\n",
        "        self.qae_model.eval()\n",
        "\n",
        "    def encode_qa_pairs(self, questions: List[str], answers: List[str]) -> List[torch.tensor]:\n",
        "        \"\"\"Takes a list of questions and a list of answers and encodes them as a list of tensors.\"\"\"\n",
        "        encoded_pairs = []\n",
        "\n",
        "        for question, answer in zip(questions, answers):\n",
        "            encoded_qa = self._encode_qa(question, answer)\n",
        "            encoded_pairs.append(encoded_qa.to(self.device))\n",
        "\n",
        "        return encoded_pairs\n",
        "\n",
        "    def get_scores(self, encoded_qa_pairs: List[torch.tensor]) -> List[float]:\n",
        "        \"\"\"Generates scores for a list of encoded QA pairs.\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        for i in range(len(encoded_qa_pairs)):\n",
        "            scores[i] = self._evaluate_qa(encoded_qa_pairs[i])\n",
        "\n",
        "        return [\n",
        "            k for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        ]\n",
        "\n",
        "    def _encode_qa(self, question: str, answer: str) -> torch.tensor:\n",
        "        \"\"\"Concatenates a question and answer, and then tokenizes them. Returns a tensor of\n",
        "        input ids corresponding to indices in the vocab.\n",
        "        \"\"\"\n",
        "        if type(answer) is list:\n",
        "            for a in answer:\n",
        "                if a[\"correct\"]:\n",
        "                    correct_answer = a[\"answer\"]\n",
        "        else:\n",
        "            correct_answer = answer\n",
        "\n",
        "        return self.qae_tokenizer(\n",
        "            text=question,\n",
        "            text_pair=correct_answer,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.SEQ_LENGTH,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _evaluate_qa(self, encoded_qa_pair: torch.tensor) -> float:\n",
        "        \"\"\"Takes an encoded QA pair and returns a score.\"\"\"\n",
        "        output = self.qae_model(**encoded_qa_pair)\n",
        "        return output[0][0][1]\n",
        "\n",
        "\n",
        "def print_qa(qa_list: List[Mapping[str, str]], show_answers: bool = True) -> None:\n",
        "    \"\"\"Formats and prints a list of generated questions and answers.\"\"\"\n",
        "\n",
        "    for i in range(len(qa_list)):\n",
        "        # wider space for 2 digit q nums\n",
        "        space = \" \" * int(np.where(i < 9, 3, 4))\n",
        "\n",
        "        print(f\"{i + 1}) Q: {qa_list[i]['question']}\")\n",
        "\n",
        "        answer = qa_list[i][\"answer\"]\n",
        "\n",
        "        # print a list of multiple choice answers\n",
        "        if type(answer) is list:\n",
        "\n",
        "            if show_answers:\n",
        "                print(\n",
        "                    f\"{space}A: 1. {answer[0]['answer']} \"\n",
        "                    f\"{np.where(answer[0]['correct'], '(correct)', '')}\"\n",
        "                )\n",
        "                for j in range(1, len(answer)):\n",
        "                    print(\n",
        "                        f\"{space + '   '}{j + 1}. {answer[j]['answer']} \"\n",
        "                        f\"{np.where(answer[j]['correct']==True,'(correct)', '')}\"\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                print(f\"{space}A: 1. {answer[0]['answer']}\")\n",
        "                for j in range(1, len(answer)):\n",
        "                    print(f\"{space + '   '}{j + 1}. {answer[j]['answer']}\")\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "        # print full sentence answers\n",
        "        else:\n",
        "            if show_answers:\n",
        "                print(f\"{space}A: {answer}\\n\")"
      ],
      "metadata": {
        "id": "QRdhXat6a1Qc"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}
